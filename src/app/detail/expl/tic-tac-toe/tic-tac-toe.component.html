<p>
This time, my goal was to create a neural network that plays Tic-Tac-Toe. It is one of the simplest games imaginable, easily understood even by small children. It is also fully solved, meaning that the perfect move for a given board is known, unlike chess or Go for example. <a href="http://www.mathrec.org/old/2002jan/solutions.html">This article</a> shows that there are only 255,168 moves, so indeed easily brute forceable.
</p>
<p>
When working with board states we can consider the symmetries of each board. A board can be rotated and flipped but that does not change the decision for the perfect move (just that it also needs to be rotated or flipped). Knowing that we can reduce the state space even more. The code for generating these boards is taken from this <a href="https://github.com/nfmcclure/tensorflow_cookbook/tree/master/06_Neural_Networks/08_Learning_Tic_Tac_Toe">GitHub repo</a>. I extended the states given by the author, because they only considered perfect moves. This meant that the AI could easily be tricked by not playing perfect moves, so I added my own. Right now there are only approximately 50 boards, the AI learns from. That’s a tiny amount considering its performance.
</p>

<p>
The neural network learns very fast from the generated data (see loss). After only 10-15 seconds (on my machine) it is basically unbeatable. So you probably got only a couple of chances at beating it. You will soon realise that it might still forego the chance for a win. Thats because I made it taunt you. Just kidding its because it hasn’t learnt how to respond to imperfect moves. I could add more moves but I think thats unnecessary.
</p>

<p>
This time we need to perform a special operation on the labels. The board is indexed like so: </p><img [src]="boardInices" style="width:150px; width:150px"> <p>For each board we already got the desired move. I designed the network to have 9 output nodes so that each index of the output-tensor represents an index of the board. The index with the highest value in the output is chosen. In order to calculate the loss with the predefined algorithms, the output (1d tensor with 9 elements) and the labels (index between 0 and 8) need to be comparable. This is a common problem so TensorflowJS has a function to do that. It takes a number and outputs a unique vector for that number with the desired shape. This process is called one hot encoding. Each element from the encoded vector is zero except at the position of the input value. Mathematically speaking it generates the i-th <a href="https://en.wikipedia.org/wiki/Standard_basis">Standard Basis Vector</a> for input i, if you are familiar with vector spaces.
</p>

<p>
Internally the neural network has two dense layers with 100 nodes each. For the activation function, I use ‘relu’ for the first time. It is defined like so:
</p>
<ng-katex [equation]="relu"></ng-katex>

<p>
From the definition we can see it is 0 for every x smaller than 0 and x for every x greater or equal than 0.
</p>
